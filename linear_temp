---
title: 'Verkefni 5'
author: 'Jaan Jaerving (jaj20@hi.is)'
output:
  rmdformats::readthedown:
    highlight: kate
    code_folding: show
---

```{r, setup, results = FALSE, message = FALSE, warning = FALSE}
options(scipen = 999)
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

library(ggplot2)
library(dplyr)
library(GGally)
library(MASS)
library(stats)
library(olsrr)
```

## 1 Read the data, subset out the areas of interest and do some initial cleaning

```{r}
data <-
  read.table(
    '/home/anjrv/Projects/linear_statistical_models/data/gagnasafn_endurmat2017_litid.csv',
    header = TRUE,
    fill = FALSE,
    sep = ',',
    dec = '.',
    na.strings = c('', ' ', 'NA' , '*'),
    stringsAsFactors = TRUE
  )

# Subset areas:
# (i)   Vesturbær: Vestan Bræðraborgarstígs   Maps to 11
# (ii)  Miðbær frá Bræðraborgarstíg að Tjörn  Maps to 25
# (iii) Háleiti/Skeifa                        Maps to 91
# (iv)  Grafarvogur: Hamrar, Foldir, Hús      Maps to 120
# (v)   Hólar, Berg                           Maps to 160

areas <- c(11, 25, 91, 120, 160)
data <- data[data$matssvaedi %in% areas, ]

rm(areas)

head(data, 5)
```

We can read from the documentation that some variables reflect the same properties of the asset, `ibteg` for example is a simplification of `teg_eign` based on whether or not the property can be considered single-family detached or multi-family residential. These variables should therefore not be used in tandem. Additionally some variables for the areas selected are variables that present no variation and would not be useful for the model.

```{r}
data['svfn'] %>%
  distinct() %>%
  knitr::kable()
```

We can also use the property id column to sanity check that there are no duplicate property entries affecting our data.

```{r}
dupes <-
  data[duplicated(data$rfastnum) |
         duplicated(data$rfastnum, fromLast = TRUE), ]
dupes[order(dupes$rfastnum, dupes$kdagur), ] %>%
  head(5) %>%
  knitr::kable(caption = 'Sample duplicate sales of the same property')
```

As seen there can be multiple sale entries for the same property. This is effectively a measurement of inflation rather than any of the features of the apartment so it is likely best to keep the newest purchase dates where duplicates exist.

```{r}
keep <- dupes %>%
  group_by(rfastnum) %>%
  slice_max(order_by = kdagur, n = 1)

delete <- anti_join(dupes, keep, by = c('rfastnum', 'kdagur'))

rm(dupes)
rm(keep)

data <- data %>% anti_join(delete)

rm(delete)
```

Finally we can remove the noted aforementioned columns before moving forward.

```{r}
drop <- c('svfn', 'kdagur')
data <- data[,!(colnames(data) %in% drop)]
data <- na.omit(data)

data$ibteg <- as.factor(data$ibteg)
data$teg_eign <- as.factor(data$teg_eign)
data$matssvaedi <- as.factor(data$matssvaedi)
data$undirmatssvaedi <- as.factor(data$undirmatssvaedi)

rm(drop)
```

## 2 Split the remaining data into training and test sets

```{r}
sample_rows <- floor(0.75 * nrow(data))

set.seed(35)
idx <- sample(seq_len(nrow(data)), size = sample_rows)

train <- data[idx,]
test <- data[-idx,]

rm(idx)
rm(data)
rm(sample_rows)
```

## 3 Train data using steps discussed in Statistical Strategy

*Initial factor visualization*

```{r}
ggplot(train) +
  geom_point(aes(
    x = ibm2,
    y = nuvirdi,
    colour = teg_eign,
    na.rm = TRUE
  )) +
  facet_grid( ~ matssvaedi) +
  theme(legend.position = 'bottom') +
  labs(x = 'Stærð (Fermetrar)', y = 'Kaupverð (Í þúsundum króna)', colour =
         'Tegund:')
```

To start with we can create a model with all the variables to see what we get

```{r}
lm.1 <- lm(nuvirdi ~ . - rfastnum, train)
s.1 <- summary(lm.1)

s.1
```



Somewhat unsurprisingly the variables for area, type of property and size are dominant initially. We can also see that some variables shift much more aggressively than others so a transformation is likely wise.

```{r}
boxcox(lm.1, lambda = seq(-0.5, 0.5, by = 0.1))
```

From the boxcox plot we see that 0 is not within the confidence interval so we cannot use log directly but we see that $\lambda$ is approximately 0.1 which suggests a transformation of the form

$$y^{0.1}$$

It seems that it could be ideal to also log transform the size of the properties or `ibm2`. As such we can do some variable selection starting with the model `lm(nuvirdi ^ 0.1 ~ . + log(ibm2)- rfastnum - ibm2, train)`

```{r}
train <- train %>%
  mutate(age_category = ifelse(byggar < 1970, 1, ifelse(byggar > 1985, 3, 2)))

train$age_category <- as.factor(train$age_category)

lm.mid <- lm(nuvirdi ^ -0.1 ~ age_category + lyfta + log(ibm2) + fjbilast + fjgeym + matssvaedi + undirmatssvaedi + teg_eign, train)

summary(lm.mid)
AIC(lm.mid)

ols_coll_diag(lm.mid)

# Remove in order:
# fjklos        -4784.464    4.234    15.931    0.79005      0.78651 
# stig10        -4786.242    4.234    15.930    0.79002      0.78662 
# fjeld         -4787.891    4.235    15.929    0.78997      0.78671 
# fjbkar        -4789.089    4.237    15.927    0.78986      0.78674 
# haednr        -4789.913    4.240    15.924    0.78971      0.78672 
# fjhaed        -4790.226    4.245    15.920    0.78948      0.78663

# Double check transform, reapply
# fjsturt       -11186.617    0.073     0.259    0.78098      0.77815 
# fjherb        -11187.110    0.073     0.259    0.78077      0.77808

# Checking for colinear variables we confirm that teg_eign and ibteg represent the same information so we compare models where we use only one of these two.

# Model with ibteg: AIC -11147.24, Adj R^2 0.772, Std. err 0.00694
# Model with teg_eign: AIC -11171.76, Adj R^2 0.7758, Std. err 0.006881
# Model with teg_eign appears to perform better
# It also appears likely that ibm2 and fjstof are colinear
# Removing fjstof results in, minimal loss so we go with this
# AIC -11172.25, Adj R^2 0.7757, Std. err 0.006882
# Properties of byggar also appear somewhat strange, lets try to factorize this similar to what was suggested in the model construction chapter of the data documentation.
# AIC -11184.5, Adj R^2 0.7774, Std. err 0.006856
# This appears to be a positive change
# Now that we have our variables we can double check our suggested transform and now boxcox lambda is effectively near zero so we log transform nuvirdi
```

Applying this transformation to the response is not satisfactory for the higher price categories however. Eventually it was better to resolve issues within the fitted vs response plot by log transforming ibm2 at which point the suggested transform from boxcox was to rather apply a negative -0.1 power.

Variable selection was iteratively performed using the `AIC()` function from the stats library as well as observing the p-values, RMSE and R-squared. The following model is what the process produced.

```{r}
lm.2 <-
  lm(
    nuvirdi ^ -0.1 ~ log(ibm2) + fjbilast + fjstof + byggar + matssvaedi + undirmatssvaedi + ibteg,
    train
  )
s.2 <- summary(lm.2)

s.2
```

In addition to the above summary This current selection of variables also resulted in an AIC of `r AIC(lm.2)`

```{r}
diag <- fortify(lm.mid)
p <- ggplot(diag, aes(x = .fitted, y = .resid)) + geom_point()
p <- p + stat_smooth(method = 'loess', se = F) +
  geom_hline(yintercept = 0,
             col = 'red',
             linetype = 'dashed')
p + xlab('Fitted') + ylab('Residuals')

rm(diag)
rm(p)
```

## 4 Use test data to evaluate the model

### Predicting test data

Since the model uses a transformed response variable as well as a transformed prediction for `ibm2` it was necessary to scale these in the test set as well to compare prediction capabilities. The comparative RMSE and the original R-squared can be seen in the following table.

```{r}
test$nuvirdi <- test$nuvirdi ^ -0.1
# test['log(ibm2)'] <- log(test$ibm2)
test <- test %>%
  mutate(age_category = ifelse(byggar < 1970, 1, ifelse(byggar > 1985, 3, 2)))

test$age_category <- as.factor(test$age_category)

predictions <- predict(lm.mid, test)
predictions <- predictions ^ (1/-0.1)
test$nuvirdi <- test$nuvirdi ^ (1/-0.1)

rmse <- sqrt(sum(predictions - test$nuvirdi) ^ 2) / length(test$nuvirdi)
c(RMSE = rmse, R2 = summary(lm.2)$r.squared) %>%
  knitr::kable(col.names = NULL)
```

This initial model appears to perform reasonable for the purpose of predicting the existing values within the test set. It is likely it could benefit from some additional diagnostics however, from the following plot it seems rather likely that outlier values exist at the high end of the price scale.

```{r}
test$predictions <- predictions

p1 <- ggplot(test, aes(x = nuvirdi, y = predictions)) + geom_point()
p1 <- p1 + stat_smooth(method = 'loess', se = F)
p1 <- p1 + xlab('Real') + ylab('Prediction')
p1 + xlim(0, 110000)+ylim(0, 110000)

rm(predictions)
rm(p1)
```


### Additional diagnostics

## 5 Equation for the final model
