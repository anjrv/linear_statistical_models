---
title: 'Verkefni 5'
author: 'Jaan Jaerving (jaj20@hi.is)'
output:
  rmdformats::readthedown:
    highlight: kate
    code_folding: show
---

```{r, setup, results = FALSE, message = FALSE, warning = FALSE}
options(scipen = 999)
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

library(ggplot2)
library(dplyr)
library(GGally)
library(MASS)
library(stats)
library(olsrr)
```

## 1 Read the data, subset out the areas of interest and do some initial cleaning

```{r}
data <-
  read.table(
    '/home/anjrv/Projects/linear_statistical_models/data/gagnasafn_endurmat2017_litid.csv',
    header = TRUE,
    fill = FALSE,
    sep = ',',
    dec = '.',
    na.strings = c('', ' ', 'NA' , '*'),
    stringsAsFactors = TRUE
  )

# Subset areas:
# (i)   Vesturbær: Vestan Bræðraborgarstígs   Maps to 11
# (ii)  Miðbær frá Bræðraborgarstíg að Tjörn  Maps to 25
# (iii) Háleiti/Skeifa                        Maps to 91
# (iv)  Grafarvogur: Hamrar, Foldir, Hús      Maps to 120
# (v)   Hólar, Berg                           Maps to 160

areas <- c(11, 25, 91, 120, 160)
data <- data[data$matssvaedi %in% areas, ]

rm(areas)

head(data, 5)
```

A quick look at the data columns shows reveals that there are some columns that are not useful for building a model such as the ID column of the properties `rfastnum` as well as some other columns that will not be useful without transforming such as the property purchase dates `kdagur`. We can start by parsing the dates in `kdagur` and scaling them around a median.

```{r}
dates <- as.numeric(as.Date(data$kdagur, format="%Y-%m-%d"))
date_mid <- median(dates)
dates <- scale(dates - date_mid)
data$date_modifier <- dates[,1]

rm(date_mid)
rm(dates)
```

Additionally there are some columns that, for our selected areas, are effectively constants as can be seen for the `svfn` column. This column can be safely removed.

```{r}
data['svfn'] %>%
  distinct() %>%
  knitr::kable()
```

Reading the documentation it also appeared reasonable to break down the building years in a similar manner. This also turned out to be a positive change with regards to model performance later so to condense the report this is included in the initial data section.

```{r}
data <- data %>%
  mutate(age_category = ifelse(byggar < 1970, 1, ifelse(byggar > 1985, 3, 2)))

drop <- c('svfn', 'kdagur', 'byggar')
data <- data[,!(colnames(data) %in% drop)]
data <- na.omit(data)

data$ibteg <- as.factor(data$ibteg)
data$teg_eign <- as.factor(data$teg_eign)
data$matssvaedi <- as.factor(data$matssvaedi)
data$undirmatssvaedi <- as.factor(data$undirmatssvaedi)

rm(drop)
```

## 2 Split the remaining data into training and test sets

We use a random seed and the number of rows to split into test and train sets.

```{r}
sample_rows <- floor(0.75 * nrow(data))

set.seed(35)
idx <- sample(seq_len(nrow(data)), size = sample_rows)

train <- data[idx,]
test <- data[-idx,]

rm(idx)
rm(data)
rm(sample_rows)
```

## 3 Train data using steps discussed in Statistical Strategy

To get a feel for the data it felt beneficial to plot out some of the variables and factors that were likely to be important. Specifically area, size and type of housing. In the graph below it is quite evident that detached housing comes at a premium compared to multi family housing.

```{r}
ggplot(train) +
  geom_point(aes(
    x = ibm2,
    y = nuvirdi,
    colour = teg_eign,
    na.rm = TRUE
  )) +
  facet_grid( ~ matssvaedi) +
  theme(legend.position = 'bottom') +
  labs(x = 'Stærð (Fermetrar)', y = 'Kaupverð (Í þúsundum króna)', colour =
         'Tegund:')
```

The next step is to fit an initial tentative model to get a feel for any initial transformations and variables.

```{r}
lm.all <- lm(nuvirdi ~ . - rfastnum, train)
s.all <- summary(lm.all)

s.all
```

There are some good candidates here to be pruned but before we get into variable selection it is prudent to check for an initial perform.

```{r}
boxcox(lm.all, lambda = seq(-0.5, 0.5, by = 0.1))
```
$\lambda$ is quite close to 0 for this initial check and it is also within the confidence interval. For initial variable selection a log transform can then be applied.

```{r}
lm.all <- lm(log(nuvirdi) ~ . - rfastnum, train)
s.all <- summary(lm.all)

s.all
```

Starting from the model above Stepwise AIC Backward Regression was applied. The table below shows the variables that were removed throughout the first iteration.

```{r}
data.frame(variable = c("fjsturt","stig10","fjgeym","fjbkar","lyfta"), aic = c(-1504.682,-1506.093,-1507.257,-1508.270,-1508.379), adj_r_sq = c(0.84697,0.84701,0.84702,0.84703,0.84695)) %>%
  knitr::kable(caption = "Initially removed variables", col.names = c("Variable", "AIC", "Adj. R-Sq"))

# No further variables could be removed with backward elimination
# The next step was to check for colinear variables we see that a large portion of the information provided by fjherb is covered by ibm2. The same can be seen for the relationship between ibteg and teg_eign. These models were tested against each other and the better performing model was kept. This was repeated again and it was revealed that the type of property also had a weaker colinear relationship with the number of floors.
```

After this process it became apparent that log transforming the property size variable `ibm2` would help with the accuracy of the model, checking the performance of the variables after this resulted in some additional removals which are shown below.

```{r}
data.frame(variable = c("fjeld","fjklos"), aic = c(-1921.173, -1921.734), adj_r_sq = c(0.87886, 0.87883)) %>%
  knitr::kable(caption = "Variables removed after transforming size", col.names = c("Variable", "AIC", "Adj. R-Sq"))
```

After this step the remaining variables were inspected for collinearity.

```{r}
lm.vars <- lm(log(nuvirdi) ~ teg_eign + log(ibm2) + fjbilast + fjstof + matssvaedi + undirmatssvaedi, train)
s.vars <- summary(lm.vars)

s.vars
```

Double checking the transform after selecting all these variables using `boxcox` shows that $\lambda$ is still close to 0 and 0 is within the confidence interval, so we keep the log transform for our response variable.

```{r}
boxcox(lm(nuvirdi ~ teg_eign + log(ibm2) + fjbilast + fjstof + matssvaedi + undirmatssvaedi, train), lambda = seq(-0.5, 0.5, by = 0.1))
```

## 4 Use test data to evaluate the model


### Additional diagnostics

## 5 Equation for the final model