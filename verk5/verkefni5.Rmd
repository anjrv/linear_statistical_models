---
title: 'Verkefni 5'
author: 'Jaan Jaerving (jaj20@hi.is)'
output:
  rmdformats::readthedown:
    highlight: kate
    code_folding: show
---

```{r, setup, results = FALSE, message = FALSE, warning = FALSE}
options(scipen = 999)
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

library(ggplot2)
library(dplyr)
library(GGally)
library(MASS)

aic <- function(x) {
  n <- nrow(x$model)
  val <- n * log((sum(x$residuals ^ 2) / n)) + (length(x$coefficients) *
                                                  2)
  return(val)
}
```

## 1 Read the data, subset out the areas of interest and do some initial cleaning

```{r}
data <-
  read.table(
    '/home/anjrv/Projects/linear_statistical_models/data/gagnasafn_endurmat2017_litid.csv',
    header = TRUE,
    fill = FALSE,
    sep = ',',
    dec = '.',
    na.strings = c('', ' ', 'NA' , '*'),
    stringsAsFactors = TRUE
  )

# Subset areas:
# (i)   Vesturbær: Vestan Bræðraborgarstígs   Maps to 11
# (ii)  Miðbær frá Bræðraborgarstíg að Tjörn  Maps to 25
# (iii) Háleiti/Skeifa                        Maps to 91
# (iv)  Grafarvogur: Hamrar, Foldir, Hús      Maps to 120
# (v)   Hólar, Berg                           Maps to 160

areas <- c(11, 25, 91, 120, 160)
data <- data[data$matssvaedi %in% areas, ]

rm(areas)

head(data, 5)
```

We can read from the documentation that some variables reflect the same properties of the asset, `ibteg` for example is a simplification of `teg_eign` based on whether or not the property can be considered single-family detached or multi-family residential. These variables should therefore not be used in tandem. Additionally some variables for the areas selected are variables that present no variation and would not be useful for the model.

```{r}
data['svfn'] %>%
  distinct() %>%
  knitr::kable()
```

We can also use the property id column to sanity check that there are no duplicate property entries affecting our data.

```{r}
dupes <-
  data[duplicated(data$rfastnum) |
         duplicated(data$rfastnum, fromLast = TRUE), ]
dupes[order(dupes$rfastnum, dupes$kdagur), ] %>%
  head(5) %>%
  knitr::kable(caption = 'Sample duplicate sales of the same property')
```

As seen there can be multiple sale entries for the same property. This is effectively a measurement of inflation rather than any of the features of the apartment so it is likely best to keep the newest purchase dates where duplicates exist.

```{r}
keep <- dupes %>%
  group_by(rfastnum) %>%
  slice_max(order_by = kdagur, n = 1)

delete <- anti_join(dupes, keep, by = c('rfastnum', 'kdagur'))

rm(dupes)
rm(keep)

data <- data %>% anti_join(delete)

rm(delete)
```

Finally we can remove the noted aforementioned columns before moving forward.

```{r}
drop <- c('svfn', 'kdagur')
data <- data[,!(colnames(data) %in% drop)]
data <- na.omit(data)

data$ibteg <- as.factor(data$ibteg)
data$teg_eign <- as.factor(data$teg_eign)
data$matssvaedi <- as.factor(data$matssvaedi)
data$undirmatssvaedi <- as.factor(data$undirmatssvaedi)

rm(drop)
```

## 2 Split the remaining data into training and test sets

```{r}
sample_rows <- floor(0.75 * nrow(data))

set.seed(12)
idx <- sample(seq_len(nrow(data)), size = sample_rows)

train <- data[idx,]
test <- data[-idx,]

rm(idx)
rm(data)
rm(sample_rows)
```

## 3 Train data using steps discussed in Statistical Strategy

*Initial factor visualization*

```{r}
ggplot(train) +
  geom_point(aes(
    x = ibm2,
    y = nuvirdi,
    colour = teg_eign,
    na.rm = TRUE
  )) +
  facet_grid( ~ matssvaedi) +
  theme(legend.position = 'bottom') +
  labs(x = 'Stærð (Fermetrar)', y = 'Kaupverð (Í þúsundum króna)', colour =
         'Tegund:')
```

To start with we can create a model with all the variables to see what we get

```{r}
lm.1 <- lm(nuvirdi ~ . - rfastnum, train)
s.1 <- summary(lm.1)

s.1
```

Somewhat unsurprisingly the variables for area, type of property and size are dominant initially. We can also see that some variables shift much more aggressively than others so a transformation is likely wise.

```{r}
boxcox(lm.1, lambda = seq(0.0, 1.0, by = 0.1))
```

From the boxcox plot we see that 0 is not within the confidence interval so we cannot use log directly but we see that $\lambda$ is approximately 0.1 which suggests a transformation of the form

$$y^{0.1}$$
Applying this transformation to the response is not satisfactory for the higher price categories however. Eventually it was better to resolve issues within the fitted vs response plot by log transforming ibm2 at which point the suggested transform from boxcox was to rather apply a negative -0.1 power.

Variable selection was iteratively performed using the `aic()` function defined earlier in the document as well as observing the p-values, RMSE and R-squared. The following model is what the process produced.

```{r}
lm.2 <-
  lm(
    nuvirdi ^ -0.1 ~ log(ibm2) + fjbilast + fjstof + byggar + matssvaedi + undirmatssvaedi + ibteg,
    train
  )
s.2 <- summary(lm.2)

s.2
```

In addition to the above summary This current selection of variables also resulted in an AIC of `r aic(lm.2)`

```{r}
diag <- fortify(lm.2)
p <- ggplot(diag, aes(x = .fitted, y = .resid)) + geom_point()
p <- p + stat_smooth(method = 'loess', se = F) +
  geom_hline(yintercept = 0,
             col = 'red',
             linetype = 'dashed')
p + xlab('Fitted') + ylab('Residuals')

rm(diag)
rm(p)
```

## 4 Use test data to evaluate the model

### Predicting test data

Since the model uses a transformed response variable as well as a transformed prediction for `ibm2` it was necessary to scale these in the test set as well to compare prediction capabilities. The comparative RMSE and the original R-squared can be seen in the following table.

```{r}
test$nuvirdi <- test$nuvirdi ^ -0.1
test['log(ibm2)'] <- log(test$ibm2)

predictions <- predict(lm.2, test)
predictions <- predictions ^ (1/-0.1)
test$nuvirdi <- test$nuvirdi ^ (1/-0.1)

rmse <- sqrt(sum(predictions - test$nuvirdi) ^ 2) / length(test$nuvirdi)
c(RMSE = rmse, R2 = summary(lm.2)$r.squared) %>%
  knitr::kable(col.names = NULL)
```



```{r}
test$predictions <- predictions

p1 <- ggplot(test, aes(x = nuvirdi, y = predictions)) + geom_point()
p1 <- p1 + stat_smooth(method = 'loess', se = F)
p1 <- p1 + xlab('Real') + ylab('Prediction')
p1 + xlim(0, 110000)+ylim(0, 110000)

rm(predictions)
rm(p1)
```

This initial model appears to perform reasonable for the purpose of predicting the existing values within the test set. It is likely it could benefit from some additional diagnostics however, from previous plots it seems rather likely that outlier values exist at the high end of the price scale.

### Additional diagnostics

## 5 Equation for the final model